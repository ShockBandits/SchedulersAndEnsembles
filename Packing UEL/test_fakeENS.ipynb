{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from fakeENS.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from fakeENS import fakeENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_arr(arr, mode = 'float'):\n",
    "    if mode == 'int':\n",
    "        return \" \".join(\"%d\"%x for x in arr)\n",
    "    else:\n",
    "        return \" \".join(\"%.3f\"%x for x in arr)\n",
    "\n",
    "def averageAcc(p_true, C):\n",
    "    return np.sum(C[i,i]*p for i, p in enumerate(p_true))    \n",
    "\n",
    "def norm_row(M):\n",
    "    row_sums = M.sum(axis=1).astype('float')\n",
    "    new_M = M /row_sums[:, np.newaxis]\n",
    "    return new_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 5\n",
    "p_true = np.random.rand(num_class); p_true = p_true/ np.sum(p_true)\n",
    "\n",
    "num_classifier = 7\n",
    "\n",
    "num_tot_samp = 10000\n",
    "stream_sample = np.arange(num_tot_samp)+1\n",
    "stream_true_label = np.random.choice(num_class, num_tot_samp, p = p_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min accuracy: 0.400 0.250 0.250 0.400 0.250 0.400 0.200\n",
      "Avg accuracy: 0.522 0.389 0.421 0.529 0.417 0.516 0.364\n"
     ]
    }
   ],
   "source": [
    "# Replace with the actual Ensemble from Steven's github\n",
    "ENS = fakeENS(num_classifier, num_class, stream_true_label)\n",
    "conf_mat = ENS.getConfMat()\n",
    "avg_acc = [averageAcc(p_true, conf_mat[i]) for i in range(num_classifier)]\n",
    "\n",
    "print 'Min accuracy: %s'%str_arr(ENS.alpha)\n",
    "print 'Avg accuracy: %s'%str_arr(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_count = np.zeros((num_classifier, num_class, num_class))\n",
    "\n",
    "for i in range(num_tot_samp):\n",
    "    real_label = stream_true_label[i]\n",
    "    schedule = stream_sample[i]*np.ones(num_classifier)\n",
    "    labels = ENS.classify(schedule) - 1\n",
    "    #print \"real:\", real_label, \"labels:\", labels\n",
    "    for j in range(num_classifier): conf_count[j, real_label, int(labels[j])] += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in empirical vs true conf matrix:0.437\n"
     ]
    }
   ],
   "source": [
    "# compute empirical confusion matrices\n",
    "conf_mat_emp = {j: norm_row(conf_count[j,:,:]) for j in range(num_classifier)}\n",
    "# compute the errors\n",
    "err = np.sum(np.linalg.norm(conf_mat_emp[j] - conf_mat[j], 1)for j in range(num_classifier))\n",
    "# display error\n",
    "print 'Error in empirical vs true conf matrix:%.3f'%err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
